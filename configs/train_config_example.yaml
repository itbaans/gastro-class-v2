# Training Configuration for GastroClassTraining
# Example configuration with all available options

# ===== DATA CONFIGURATION =====
data:
  root_dir: "path/to/your/dataset"  # Required: Path to your dataset directory
  nested_classes: true  # Set to true if your data is structured as: datadir/class1/class1/images...
  image_size: 224  # Image size for resizing
  train_split: 0.8  # Train/validation split ratio
  num_workers: 4  # Number of data loading workers
  seed: 42  # Random seed for reproducibility

# ===== DATA AUGMENTATION =====
augmentation:
  horizontal_flip: 0.5  # Probability of horizontal flip
  vertical_flip: 0.3  # Probability of vertical flip
  rotation: 20  # Random rotation degrees
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
  translate: [0.1, 0.1]  # Random translation

# ===== MODEL CONFIGURATION =====
model:
  name: "resnet50"  # Options: resnet18, resnet34, resnet50, resnet101, resnet152
  num_classes: null  # Will be auto-detected from dataset
  pretrained: true  # Use ImageNet pretrained weights
  pretrained_path: null  # Path to custom pretrained weights (e.g., models/gastronet_5m.pth)
  freeze_features: false  # If true, freeze entire feature extractor (only train classifier)
  freeze_layers: 0  # Number of layer groups to freeze (0-4), 0 means no freezing

# ===== TRAINING CONFIGURATION =====
training:
  epochs: 50  # Number of training epochs
  batch_size: 32  # Batch size
  learning_rate: 0.001  # Initial learning rate
  weight_decay: 0.0001  # L2 regularization
  optimizer: "adam"  # Options: adam, sgd
  momentum: 0.9  # Only used for SGD optimizer
  
  # Learning rate scheduler
  scheduler:
    type: "step"  # Options: step, cosine, null (no scheduler)
    step_size: 10  # For StepLR: reduce LR every N epochs
    gamma: 0.1  # For StepLR: multiply LR by this factor
    min_lr: 0.000001  # For CosineAnnealingLR: minimum learning rate (1e-6)
  
  # Early stopping
  early_stopping:
    enabled: true  # Enable early stopping
    patience: 10  # Stop if no improvement for N epochs
    min_delta: 0.001  # Minimum change to qualify as improvement

# ===== CHECKPOINT CONFIGURATION =====
checkpoint:
  save_dir: "checkpoints"  # Directory to save checkpoints
  save_best: true  # Save best model based on validation accuracy
  save_every: 5  # Save checkpoint every N epochs
  resume_from: null  # Path to checkpoint to resume training from

# ===== LOGGING CONFIGURATION =====
logging:
  log_dir: "logs"  # Directory for logs
  tensorboard: false  # Enable TensorBoard logging
  print_freq: 10  # Print training stats every N batches
